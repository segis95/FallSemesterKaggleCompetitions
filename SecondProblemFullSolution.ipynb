{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transaction merge...\n",
      "user logs merge...\n",
      "members merge...\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "import gc; gc.enable()\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import *\n",
    "import sklearn\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "train = pd.concat((train, pd.read_csv('train_v2.csv')), axis=0, ignore_index=True).reset_index(drop=True)\n",
    "test = pd.read_csv('sample_submission_v2.csv')\n",
    "\n",
    "transactions = pd.read_csv('transactions.csv', usecols=['msno'])\n",
    "transactions = pd.concat((transactions, pd.read_csv('transactions_v2.csv', usecols=['msno'])), axis=0, ignore_index=True).reset_index(drop=True)\n",
    "transactions = pd.DataFrame(transactions['msno'].value_counts().reset_index())\n",
    "transactions.columns = ['msno','trans_count']\n",
    "train = pd.merge(train, transactions, how='left', on='msno')\n",
    "test = pd.merge(test, transactions, how='left', on='msno')\n",
    "transactions = []; print('transaction merge...')\n",
    "\n",
    "user_logs = pd.read_csv('user_logs_v2.csv', usecols=['msno'])\n",
    "user_logs = pd.DataFrame(user_logs['msno'].value_counts().reset_index())\n",
    "user_logs.columns = ['msno','logs_count']\n",
    "train = pd.merge(train, user_logs, how='left', on='msno')\n",
    "test = pd.merge(test, user_logs, how='left', on='msno')\n",
    "user_logs = []; print('user logs merge...')\n",
    "\n",
    "members = pd.read_csv('members_v3.csv')\n",
    "train = pd.merge(train, members, how='left', on='msno')\n",
    "test = pd.merge(test, members, how='left', on='msno')\n",
    "members = []; print('members merge...') \n",
    "gender = {'male':1, 'female':2}\n",
    "train['gender'] = train['gender'].map(gender)\n",
    "test['gender'] = test['gender'].map(gender)\n",
    "\n",
    "train = train.fillna(0)\n",
    "test = test.fillna(0)\n",
    "\n",
    "transactions = pd.read_csv('transactions_v2.csv') \n",
    "transactions = transactions.sort_values(by=['transaction_date'], ascending=[False]).reset_index(drop=True)\n",
    "transactions = transactions.drop_duplicates(subset=['msno'], keep='first')\n",
    "\n",
    "train = pd.merge(train, transactions, how='left', on='msno')\n",
    "test = pd.merge(test, transactions, how='left', on='msno')\n",
    "transactions=[]\n",
    "train = train.loc[(train['bd'] > 10) & (train['bd'] < 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000000, 9)\n",
      "... (1616917, 9)\n",
      "(10000000, 9)\n",
      "... (1533539, 9)\n",
      "(10000000, 9)\n",
      "... (1353720, 9)\n",
      "(2106543, 9)\n",
      "... (429234, 9)\n"
     ]
    }
   ],
   "source": [
    "def transform_df(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    df = df.sort_values(by=['date'], ascending=[False])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.drop_duplicates(subset=['msno'], keep='first')\n",
    "    return df\n",
    "\n",
    "def transform_df2(df):\n",
    "    df = df.sort_values(by=['date'], ascending=[False])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.drop_duplicates(subset=['msno'], keep='first')\n",
    "    return df\n",
    "\n",
    "df_iter = pd.read_csv('user_logs.csv', low_memory=False, iterator=True, chunksize=10000000)\n",
    "last_user_logs = []\n",
    "i = 0 \n",
    "for df in df_iter:\n",
    "    if i>35:\n",
    "        if len(df)>0:\n",
    "            print(df.shape)\n",
    "            p = Pool(cpu_count())\n",
    "            df = p.map(transform_df, np.array_split(df, cpu_count()))   \n",
    "            df = pd.concat(df, axis=0, ignore_index=True).reset_index(drop=True)\n",
    "            df = transform_df2(df)\n",
    "            p.close(); p.join()\n",
    "            last_user_logs.append(df)\n",
    "            print('...', df.shape)\n",
    "            df = []\n",
    "    i+=1\n",
    "last_user_logs.append(transform_df(pd.read_csv('user_logs_v2.csv')))\n",
    "last_user_logs = pd.concat(last_user_logs, axis=0, ignore_index=True).reset_index(drop=True)\n",
    "last_user_logs = transform_df2(last_user_logs)\n",
    "\n",
    "train = pd.merge(train, last_user_logs, how='left', on='msno')\n",
    "test = pd.merge(test, last_user_logs, how='left', on='msno')\n",
    "last_user_logs=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n",
    "test = test.fillna(0)\n",
    "\n",
    "cols = [c for c in train.columns if c not in ['is_churn','msno']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data for NN usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "nlr = MinMaxScaler()\n",
    "train_n = nlr.fit_transform(train[cols])\n",
    "test_n = nlr.fit_transform(test[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and training autoencoder on the train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 775599 samples, validate on 907471 samples\n",
      "Epoch 1/100\n",
      " - 15s - loss: 0.1387 - val_loss: 0.1038\n",
      "Epoch 2/100\n",
      " - 15s - loss: 0.0532 - val_loss: 0.0673\n",
      "Epoch 3/100\n",
      " - 15s - loss: 0.0371 - val_loss: 0.0626\n",
      "Epoch 4/100\n",
      " - 15s - loss: 0.0349 - val_loss: 0.0608\n",
      "Epoch 5/100\n",
      " - 15s - loss: 0.0341 - val_loss: 0.0599\n",
      "Epoch 6/100\n",
      " - 15s - loss: 0.0336 - val_loss: 0.0595\n",
      "Epoch 7/100\n",
      " - 15s - loss: 0.0332 - val_loss: 0.0591\n",
      "Epoch 8/100\n",
      " - 15s - loss: 0.0328 - val_loss: 0.0588\n",
      "Epoch 9/100\n",
      " - 15s - loss: 0.0325 - val_loss: 0.0585\n",
      "Epoch 10/100\n",
      " - 15s - loss: 0.0322 - val_loss: 0.0582\n",
      "Epoch 11/100\n",
      " - 15s - loss: 0.0319 - val_loss: 0.0580\n",
      "Epoch 12/100\n",
      " - 15s - loss: 0.0316 - val_loss: 0.0576\n",
      "Epoch 13/100\n",
      " - 15s - loss: 0.0313 - val_loss: 0.0573\n",
      "Epoch 14/100\n",
      " - 15s - loss: 0.0309 - val_loss: 0.0570\n",
      "Epoch 15/100\n",
      " - 15s - loss: 0.0306 - val_loss: 0.0568\n",
      "Epoch 16/100\n",
      " - 15s - loss: 0.0303 - val_loss: 0.0564\n",
      "Epoch 17/100\n",
      " - 15s - loss: 0.0299 - val_loss: 0.0561\n",
      "Epoch 18/100\n",
      " - 15s - loss: 0.0296 - val_loss: 0.0557\n",
      "Epoch 19/100\n",
      " - 15s - loss: 0.0292 - val_loss: 0.0553\n",
      "Epoch 20/100\n",
      " - 15s - loss: 0.0288 - val_loss: 0.0550\n",
      "Epoch 21/100\n",
      " - 15s - loss: 0.0284 - val_loss: 0.0545\n",
      "Epoch 22/100\n",
      " - 15s - loss: 0.0280 - val_loss: 0.0541\n",
      "Epoch 23/100\n",
      " - 15s - loss: 0.0276 - val_loss: 0.0536\n",
      "Epoch 24/100\n",
      " - 15s - loss: 0.0271 - val_loss: 0.0531\n",
      "Epoch 25/100\n",
      " - 15s - loss: 0.0266 - val_loss: 0.0526\n",
      "Epoch 26/100\n",
      " - 15s - loss: 0.0261 - val_loss: 0.0521\n",
      "Epoch 27/100\n",
      " - 15s - loss: 0.0256 - val_loss: 0.0516\n",
      "Epoch 28/100\n",
      " - 15s - loss: 0.0250 - val_loss: 0.0511\n",
      "Epoch 29/100\n",
      " - 15s - loss: 0.0245 - val_loss: 0.0506\n",
      "Epoch 30/100\n",
      " - 15s - loss: 0.0239 - val_loss: 0.0500\n",
      "Epoch 31/100\n",
      " - 15s - loss: 0.0232 - val_loss: 0.0495\n",
      "Epoch 32/100\n",
      " - 15s - loss: 0.0226 - val_loss: 0.0488\n",
      "Epoch 33/100\n",
      " - 15s - loss: 0.0219 - val_loss: 0.0482\n",
      "Epoch 34/100\n",
      " - 15s - loss: 0.0212 - val_loss: 0.0475\n",
      "Epoch 35/100\n",
      " - 15s - loss: 0.0205 - val_loss: 0.0469\n",
      "Epoch 36/100\n",
      " - 15s - loss: 0.0199 - val_loss: 0.0463\n",
      "Epoch 37/100\n",
      " - 15s - loss: 0.0192 - val_loss: 0.0457\n",
      "Epoch 38/100\n",
      " - 15s - loss: 0.0186 - val_loss: 0.0450\n",
      "Epoch 39/100\n",
      " - 15s - loss: 0.0180 - val_loss: 0.0443\n",
      "Epoch 40/100\n",
      " - 15s - loss: 0.0174 - val_loss: 0.0437\n",
      "Epoch 41/100\n",
      " - 15s - loss: 0.0168 - val_loss: 0.0431\n",
      "Epoch 42/100\n",
      " - 15s - loss: 0.0163 - val_loss: 0.0424\n",
      "Epoch 43/100\n",
      " - 15s - loss: 0.0158 - val_loss: 0.0418\n",
      "Epoch 44/100\n",
      " - 15s - loss: 0.0153 - val_loss: 0.0412\n",
      "Epoch 45/100\n",
      " - 15s - loss: 0.0148 - val_loss: 0.0406\n",
      "Epoch 46/100\n",
      " - 15s - loss: 0.0143 - val_loss: 0.0399\n",
      "Epoch 47/100\n",
      " - 15s - loss: 0.0139 - val_loss: 0.0394\n",
      "Epoch 48/100\n",
      " - 15s - loss: 0.0135 - val_loss: 0.0388\n",
      "Epoch 49/100\n",
      " - 15s - loss: 0.0131 - val_loss: 0.0383\n",
      "Epoch 50/100\n",
      " - 15s - loss: 0.0127 - val_loss: 0.0377\n",
      "Epoch 51/100\n",
      " - 15s - loss: 0.0124 - val_loss: 0.0372\n",
      "Epoch 52/100\n",
      " - 15s - loss: 0.0120 - val_loss: 0.0366\n",
      "Epoch 53/100\n",
      " - 15s - loss: 0.0117 - val_loss: 0.0361\n",
      "Epoch 54/100\n",
      " - 15s - loss: 0.0114 - val_loss: 0.0356\n",
      "Epoch 55/100\n",
      " - 15s - loss: 0.0111 - val_loss: 0.0351\n",
      "Epoch 56/100\n",
      " - 15s - loss: 0.0108 - val_loss: 0.0346\n",
      "Epoch 57/100\n",
      " - 15s - loss: 0.0106 - val_loss: 0.0341\n",
      "Epoch 58/100\n",
      " - 15s - loss: 0.0103 - val_loss: 0.0336\n",
      "Epoch 59/100\n",
      " - 15s - loss: 0.0101 - val_loss: 0.0332\n",
      "Epoch 60/100\n",
      " - 15s - loss: 0.0098 - val_loss: 0.0328\n",
      "Epoch 61/100\n",
      " - 15s - loss: 0.0096 - val_loss: 0.0324\n",
      "Epoch 62/100\n",
      " - 15s - loss: 0.0094 - val_loss: 0.0319\n",
      "Epoch 63/100\n",
      " - 15s - loss: 0.0092 - val_loss: 0.0315\n",
      "Epoch 64/100\n",
      " - 15s - loss: 0.0090 - val_loss: 0.0312\n",
      "Epoch 65/100\n",
      " - 15s - loss: 0.0088 - val_loss: 0.0308\n",
      "Epoch 66/100\n",
      " - 15s - loss: 0.0087 - val_loss: 0.0304\n",
      "Epoch 67/100\n",
      " - 15s - loss: 0.0085 - val_loss: 0.0301\n",
      "Epoch 68/100\n",
      " - 15s - loss: 0.0083 - val_loss: 0.0297\n",
      "Epoch 69/100\n",
      " - 15s - loss: 0.0082 - val_loss: 0.0294\n",
      "Epoch 70/100\n",
      " - 15s - loss: 0.0080 - val_loss: 0.0291\n",
      "Epoch 71/100\n",
      " - 15s - loss: 0.0079 - val_loss: 0.0288\n",
      "Epoch 72/100\n",
      " - 15s - loss: 0.0077 - val_loss: 0.0285\n",
      "Epoch 73/100\n",
      " - 15s - loss: 0.0076 - val_loss: 0.0282\n",
      "Epoch 74/100\n",
      " - 15s - loss: 0.0075 - val_loss: 0.0279\n",
      "Epoch 75/100\n",
      " - 15s - loss: 0.0074 - val_loss: 0.0276\n",
      "Epoch 76/100\n",
      " - 15s - loss: 0.0072 - val_loss: 0.0273\n",
      "Epoch 77/100\n",
      " - 15s - loss: 0.0071 - val_loss: 0.0270\n",
      "Epoch 78/100\n",
      " - 15s - loss: 0.0070 - val_loss: 0.0268\n",
      "Epoch 79/100\n",
      " - 15s - loss: 0.0069 - val_loss: 0.0265\n",
      "Epoch 80/100\n",
      " - 15s - loss: 0.0068 - val_loss: 0.0263\n",
      "Epoch 81/100\n",
      " - 15s - loss: 0.0067 - val_loss: 0.0260\n",
      "Epoch 82/100\n",
      " - 15s - loss: 0.0066 - val_loss: 0.0258\n",
      "Epoch 83/100\n",
      " - 15s - loss: 0.0065 - val_loss: 0.0256\n",
      "Epoch 84/100\n",
      " - 15s - loss: 0.0064 - val_loss: 0.0254\n",
      "Epoch 85/100\n",
      " - 15s - loss: 0.0064 - val_loss: 0.0252\n",
      "Epoch 86/100\n",
      " - 15s - loss: 0.0063 - val_loss: 0.0250\n",
      "Epoch 87/100\n",
      " - 15s - loss: 0.0062 - val_loss: 0.0248\n",
      "Epoch 88/100\n",
      " - 15s - loss: 0.0061 - val_loss: 0.0246\n",
      "Epoch 89/100\n",
      " - 15s - loss: 0.0061 - val_loss: 0.0244\n",
      "Epoch 90/100\n",
      " - 15s - loss: 0.0060 - val_loss: 0.0242\n",
      "Epoch 91/100\n",
      " - 15s - loss: 0.0059 - val_loss: 0.0240\n",
      "Epoch 92/100\n",
      " - 15s - loss: 0.0058 - val_loss: 0.0239\n",
      "Epoch 93/100\n",
      " - 15s - loss: 0.0058 - val_loss: 0.0237\n",
      "Epoch 94/100\n",
      " - 15s - loss: 0.0057 - val_loss: 0.0235\n",
      "Epoch 95/100\n",
      " - 15s - loss: 0.0056 - val_loss: 0.0234\n",
      "Epoch 96/100\n",
      " - 15s - loss: 0.0056 - val_loss: 0.0232\n",
      "Epoch 97/100\n",
      " - 15s - loss: 0.0055 - val_loss: 0.0230\n",
      "Epoch 98/100\n",
      " - 15s - loss: 0.0055 - val_loss: 0.0229\n",
      "Epoch 99/100\n",
      " - 15s - loss: 0.0054 - val_loss: 0.0227\n",
      "Epoch 100/100\n",
      " - 15s - loss: 0.0054 - val_loss: 0.0226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcc0da33320>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "input_dim = 23  \n",
    "\n",
    "Input_l = Input(shape=(input_dim,))\n",
    "\n",
    "encoded = Dense(200, activation='relu')(Input_l)\n",
    "encoded = Dense(200, activation='relu')(encoded)\n",
    "decoded = Dense(200, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(Input_l, decoded)\n",
    "encoder = Model(Input_l, encoded)\n",
    "\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "\n",
    "autoencoder.fit(train_n, train_n,\n",
    "                epochs=100,\n",
    "                batch_size=1280,\n",
    "                validation_data=(test_n, test_n),verbose = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aut = encoder.predict(train_n)\n",
    "test_aut = encoder.predict(test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(907471, 200)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_aut.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying keras epochwise output(callback class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "class roc_callback(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self,training_data,validation_data, display):\n",
    "        self.display = display\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "        self.seen = 0\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if int(epoch) % self.display == 0:\n",
    "            y_pred_val = self.model.predict_proba(self.x_val)\n",
    "            ll = log_loss(self.y_val, y_pred_val)\n",
    "            print('\\logloss: %s ' % ll)\n",
    "            return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and training out NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "770000/775599 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8719\\logloss: 0.262842603531 \n",
      "775599/775599 [==============================] - 70s 90us/step - loss: 0.3255 - acc: 0.8720\n",
      "Epoch 2/100\n",
      "775599/775599 [==============================] - 24s 31us/step - loss: 0.2711 - acc: 0.8907\n",
      "Epoch 3/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2549 - acc: 0.8970\n",
      "Epoch 4/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2434 - acc: 0.9076\n",
      "Epoch 5/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2358 - acc: 0.9160\n",
      "Epoch 6/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2307 - acc: 0.9201\n",
      "Epoch 7/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2272 - acc: 0.9220\n",
      "Epoch 8/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2246 - acc: 0.9230\n",
      "Epoch 9/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2230 - acc: 0.9233\n",
      "Epoch 10/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2217 - acc: 0.9236\n",
      "Epoch 11/100\n",
      "770000/775599 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9239\\logloss: 0.20796729167 \n",
      "775599/775599 [==============================] - 70s 90us/step - loss: 0.2202 - acc: 0.9239\n",
      "Epoch 12/100\n",
      "775599/775599 [==============================] - 24s 31us/step - loss: 0.2195 - acc: 0.9239\n",
      "Epoch 13/100\n",
      "775599/775599 [==============================] - 26s 33us/step - loss: 0.2187 - acc: 0.9242\n",
      "Epoch 14/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2174 - acc: 0.9243\n",
      "Epoch 15/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2169 - acc: 0.9246\n",
      "Epoch 16/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2163 - acc: 0.9248\n",
      "Epoch 17/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2156 - acc: 0.9247\n",
      "Epoch 18/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2146 - acc: 0.9250\n",
      "Epoch 19/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2142 - acc: 0.9251\n",
      "Epoch 20/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2138 - acc: 0.9252\n",
      "Epoch 21/100\n",
      "770000/775599 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9255\\logloss: 0.203639951087 \n",
      "775599/775599 [==============================] - 69s 89us/step - loss: 0.2131 - acc: 0.9255\n",
      "Epoch 22/100\n",
      "775599/775599 [==============================] - 24s 31us/step - loss: 0.2129 - acc: 0.9254\n",
      "Epoch 23/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2121 - acc: 0.9254\n",
      "Epoch 24/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2116 - acc: 0.9259\n",
      "Epoch 25/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2115 - acc: 0.9259\n",
      "Epoch 26/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2107 - acc: 0.9258\n",
      "Epoch 27/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2104 - acc: 0.9260\n",
      "Epoch 28/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2098 - acc: 0.9262\n",
      "Epoch 29/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2095 - acc: 0.9262\n",
      "Epoch 30/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2092 - acc: 0.9264\n",
      "Epoch 31/100\n",
      "770000/775599 [============================>.] - ETA: 0s - loss: 0.2090 - acc: 0.9265\\logloss: 0.200361473924 \n",
      "775599/775599 [==============================] - 68s 88us/step - loss: 0.2090 - acc: 0.9265\n",
      "Epoch 32/100\n",
      "775599/775599 [==============================] - 24s 31us/step - loss: 0.2088 - acc: 0.9266\n",
      "Epoch 33/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2081 - acc: 0.9267\n",
      "Epoch 34/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2078 - acc: 0.9269\n",
      "Epoch 35/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2077 - acc: 0.9269\n",
      "Epoch 36/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2071 - acc: 0.9272\n",
      "Epoch 37/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2067 - acc: 0.9272\n",
      "Epoch 38/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2068 - acc: 0.9272\n",
      "Epoch 39/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2064 - acc: 0.9274\n",
      "Epoch 40/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2059 - acc: 0.9275\n",
      "Epoch 41/100\n",
      "770000/775599 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.9275\\logloss: 0.19777011061 \n",
      "775599/775599 [==============================] - 70s 90us/step - loss: 0.2056 - acc: 0.9275\n",
      "Epoch 42/100\n",
      "775599/775599 [==============================] - 24s 31us/step - loss: 0.2054 - acc: 0.9277\n",
      "Epoch 43/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2052 - acc: 0.9277\n",
      "Epoch 44/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2049 - acc: 0.9280\n",
      "Epoch 45/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2046 - acc: 0.9279\n",
      "Epoch 46/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2045 - acc: 0.9280\n",
      "Epoch 47/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2043 - acc: 0.9279\n",
      "Epoch 48/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2039 - acc: 0.9282\n",
      "Epoch 49/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2036 - acc: 0.9283\n",
      "Epoch 50/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2032 - acc: 0.9284\n",
      "Epoch 51/100\n",
      "770000/775599 [============================>.] - ETA: 0s - loss: 0.2030 - acc: 0.9284\\logloss: 0.195464024969 \n",
      "775599/775599 [==============================] - 70s 90us/step - loss: 0.2030 - acc: 0.9285\n",
      "Epoch 52/100\n",
      "775599/775599 [==============================] - 24s 31us/step - loss: 0.2028 - acc: 0.9287\n",
      "Epoch 53/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2027 - acc: 0.9288\n",
      "Epoch 54/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2023 - acc: 0.9287\n",
      "Epoch 55/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2021 - acc: 0.9288\n",
      "Epoch 56/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2016 - acc: 0.9290\n",
      "Epoch 57/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2015 - acc: 0.9290\n",
      "Epoch 58/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2013 - acc: 0.9290\n",
      "Epoch 59/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2010 - acc: 0.9292\n",
      "Epoch 60/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2008 - acc: 0.9292\n",
      "Epoch 61/100\n",
      "770000/775599 [============================>.] - ETA: 0s - loss: 0.2006 - acc: 0.9292\\logloss: 0.193285702114 \n",
      "775599/775599 [==============================] - 68s 88us/step - loss: 0.2008 - acc: 0.9292\n",
      "Epoch 62/100\n",
      "775599/775599 [==============================] - 24s 31us/step - loss: 0.2005 - acc: 0.9294\n",
      "Epoch 63/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2001 - acc: 0.9294\n",
      "Epoch 64/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.2000 - acc: 0.9294\n",
      "Epoch 65/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1997 - acc: 0.9295\n",
      "Epoch 66/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1995 - acc: 0.9295\n",
      "Epoch 67/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1991 - acc: 0.9295\n",
      "Epoch 68/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1991 - acc: 0.9299\n",
      "Epoch 69/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1990 - acc: 0.9297\n",
      "Epoch 70/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1987 - acc: 0.9298\n",
      "Epoch 71/100\n",
      "770000/775599 [============================>.] - ETA: 0s - loss: 0.1984 - acc: 0.9298\\logloss: 0.191237250585 \n",
      "775599/775599 [==============================] - 69s 89us/step - loss: 0.1983 - acc: 0.9298\n",
      "Epoch 72/100\n",
      "775599/775599 [==============================] - 24s 31us/step - loss: 0.1983 - acc: 0.9298\n",
      "Epoch 73/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1979 - acc: 0.9299\n",
      "Epoch 74/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1978 - acc: 0.9299\n",
      "Epoch 75/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1977 - acc: 0.9299\n",
      "Epoch 76/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1974 - acc: 0.9300\n",
      "Epoch 77/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1970 - acc: 0.9302\n",
      "Epoch 78/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1969 - acc: 0.9302\n",
      "Epoch 79/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1965 - acc: 0.9303\n",
      "Epoch 80/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1964 - acc: 0.9303\n",
      "Epoch 81/100\n",
      "770000/775599 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9304\\logloss: 0.189170267273 \n",
      "775599/775599 [==============================] - 68s 88us/step - loss: 0.1962 - acc: 0.9303\n",
      "Epoch 82/100\n",
      "775599/775599 [==============================] - 24s 31us/step - loss: 0.1959 - acc: 0.9305\n",
      "Epoch 83/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1957 - acc: 0.9303\n",
      "Epoch 84/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1955 - acc: 0.9304\n",
      "Epoch 85/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1953 - acc: 0.9306\n",
      "Epoch 86/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1952 - acc: 0.9308\n",
      "Epoch 87/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1949 - acc: 0.9307\n",
      "Epoch 88/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1949 - acc: 0.9307\n",
      "Epoch 89/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1947 - acc: 0.9306\n",
      "Epoch 90/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1943 - acc: 0.9306\n",
      "Epoch 91/100\n",
      "770000/775599 [============================>.] - ETA: 0s - loss: 0.1940 - acc: 0.9309\\logloss: 0.187116154467 \n",
      "775599/775599 [==============================] - 71s 91us/step - loss: 0.1940 - acc: 0.9309\n",
      "Epoch 92/100\n",
      "775599/775599 [==============================] - 24s 31us/step - loss: 0.1939 - acc: 0.9309\n",
      "Epoch 93/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1938 - acc: 0.9308\n",
      "Epoch 94/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1935 - acc: 0.9309\n",
      "Epoch 95/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1932 - acc: 0.9309\n",
      "Epoch 96/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1929 - acc: 0.9311\n",
      "Epoch 97/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1928 - acc: 0.9310\n",
      "Epoch 98/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1927 - acc: 0.9312\n",
      "Epoch 99/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1924 - acc: 0.9311\n",
      "Epoch 100/100\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1922 - acc: 0.9311\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-2ee35aff8599>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m           batch_size=10000,callbacks=[roc_callback(training_data=(train_aut, train['is_churn']),validation_data=(train_aut, train['is_churn']),display = 10)] )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(500, activation='relu', input_dim=200))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_aut, train['is_churn'],\n",
    "          epochs=100,\n",
    "          batch_size=10000,callbacks=[roc_callback(training_data=(train_aut, train['is_churn']),validation_data=(train_aut, train['is_churn']),display = 10)] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding some more training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "770000/775599 [============================>.] - ETA: 0s - loss: 0.1919 - acc: 0.9312\\logloss: 0.185019405493 \n",
      "775599/775599 [==============================] - 69s 89us/step - loss: 0.1919 - acc: 0.9313\n",
      "Epoch 2/50\n",
      "775599/775599 [==============================] - 24s 31us/step - loss: 0.1917 - acc: 0.9313\n",
      "Epoch 3/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1912 - acc: 0.9313\n",
      "Epoch 4/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1913 - acc: 0.9313\n",
      "Epoch 5/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1910 - acc: 0.9313\n",
      "Epoch 6/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1908 - acc: 0.9313\n",
      "Epoch 7/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1907 - acc: 0.9313\n",
      "Epoch 8/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1904 - acc: 0.9313\n",
      "Epoch 9/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1902 - acc: 0.9315\n",
      "Epoch 10/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1901 - acc: 0.9315\n",
      "Epoch 11/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1899 - acc: 0.9315\n",
      "Epoch 12/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1896 - acc: 0.9315\n",
      "Epoch 13/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1897 - acc: 0.9315\n",
      "Epoch 14/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1893 - acc: 0.9316\n",
      "Epoch 15/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1891 - acc: 0.9315\n",
      "Epoch 16/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1889 - acc: 0.9317\n",
      "Epoch 17/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1886 - acc: 0.9318\n",
      "Epoch 18/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1885 - acc: 0.9317\n",
      "Epoch 19/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1882 - acc: 0.9317\n",
      "Epoch 20/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1879 - acc: 0.9319\n",
      "Epoch 21/50\n",
      "770000/775599 [============================>.] - ETA: 0s - loss: 0.1878 - acc: 0.9319\\logloss: 0.181100234837 \n",
      "775599/775599 [==============================] - 69s 89us/step - loss: 0.1877 - acc: 0.9319\n",
      "Epoch 22/50\n",
      "775599/775599 [==============================] - 24s 31us/step - loss: 0.1875 - acc: 0.9319\n",
      "Epoch 23/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1874 - acc: 0.9319\n",
      "Epoch 24/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1873 - acc: 0.9320\n",
      "Epoch 25/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1872 - acc: 0.9319\n",
      "Epoch 26/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1869 - acc: 0.9320\n",
      "Epoch 27/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1867 - acc: 0.9319\n",
      "Epoch 28/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1865 - acc: 0.9321\n",
      "Epoch 29/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1861 - acc: 0.9322\n",
      "Epoch 30/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1862 - acc: 0.9320\n",
      "Epoch 31/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1860 - acc: 0.9321\n",
      "Epoch 32/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1858 - acc: 0.9320\n",
      "Epoch 33/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1856 - acc: 0.9322\n",
      "Epoch 34/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1855 - acc: 0.9321\n",
      "Epoch 35/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1853 - acc: 0.9322\n",
      "Epoch 36/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1850 - acc: 0.9323\n",
      "Epoch 37/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1849 - acc: 0.9322\n",
      "Epoch 38/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1847 - acc: 0.9321\n",
      "Epoch 39/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1846 - acc: 0.9323\n",
      "Epoch 40/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1845 - acc: 0.9322\n",
      "Epoch 41/50\n",
      "770000/775599 [============================>.] - ETA: 0s - loss: 0.1842 - acc: 0.9322\\logloss: 0.17783093003 \n",
      "775599/775599 [==============================] - 70s 90us/step - loss: 0.1842 - acc: 0.9322\n",
      "Epoch 42/50\n",
      "775599/775599 [==============================] - 24s 31us/step - loss: 0.1840 - acc: 0.9323\n",
      "Epoch 43/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1842 - acc: 0.9323\n",
      "Epoch 44/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1840 - acc: 0.9324\n",
      "Epoch 45/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1836 - acc: 0.9324\n",
      "Epoch 46/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1834 - acc: 0.9323\n",
      "Epoch 47/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1834 - acc: 0.9326\n",
      "Epoch 48/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1833 - acc: 0.9324\n",
      "Epoch 49/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1830 - acc: 0.9325\n",
      "Epoch 50/50\n",
      "775599/775599 [==============================] - 25s 32us/step - loss: 0.1830 - acc: 0.9326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcc0b705940>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_aut, train['is_churn'],\n",
    "          epochs=50,\n",
    "          batch_size=10000,callbacks=[roc_callback(training_data=(train_aut, train['is_churn']),validation_data=(train_aut, train['is_churn']),display = 20 )])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making prediction and saving the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = model.predict(test_aut)\n",
    "test['is_churn'] = pred1.clip(0.+1e-15, 1-1e-15)\n",
    "test[['msno','is_churn']].to_csv('NNAEsub.csv.gz', index=False, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
